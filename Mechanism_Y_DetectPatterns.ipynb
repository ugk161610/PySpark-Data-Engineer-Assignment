{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import col, avg, count, lit, current_timestamp\n","\n","\n","jdbc_url = \"jdbc:mysql://localhost:3306/transaction_db\"\n","connection_properties = {\n","    \"user\": \"root\",\n","    \"password\": \"Gopi@777\",\n","    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n","}\n","\n","\n","importance_df = spark.read.option(\"header\", True).csv(\"Files/raw_data/CustomerImportance.csv\")\n","\n","\n","chunk_files_df = spark.read.format(\"binaryFile\").load(\"Files/chunks/\")\n","chunk_files = [row.path for row in chunk_files_df.select(\"path\").collect()]\n","\n","\n","detections = []\n","\n","\n","for file_path in chunk_files:\n","    chunk_df = spark.read.parquet(file_path)\n","    joined_df = chunk_df.join(importance_df, on=\"customerId\", how=\"left\")\n","\n","\n","    merchant_counts = joined_df.groupBy(\"merchantId\").agg(count(\"*\").alias(\"txn_count\"))\n","    merchant_counts_filtered = merchant_counts.filter(col(\"txn_count\") > 50000)\n","\n","\n","    merchant_counts.write.jdbc(\n","        url=jdbc_url,\n","        table=\"merchant_txn_counts\",\n","        mode=\"append\",\n","        properties=connection_properties\n","    )\n","\n","    for row in merchant_counts_filtered.collect():\n","        merchant_id = row[\"merchantId\"]\n","        merchant_df = joined_df.filter(col(\"merchantId\") == merchant_id)\n","        txn_by_customer = merchant_df.groupBy(\"customerId\").agg(count(\"*\").alias(\"txn_count\"), avg(\"weight\").alias(\"avg_weight\"))\n","\n","        top_10 = txn_by_customer.approxQuantile(\"txn_count\", [0.9], 0.01)[0]\n","        bottom_10_weight = txn_by_customer.approxQuantile(\"avg_weight\", [0.1], 0.01)[0]\n","\n","        upgrade_df = txn_by_customer.filter((col(\"txn_count\") >= top_10) & (col(\"avg_weight\") <= bottom_10_weight))\n","        upgrade_df = upgrade_df.withColumn(\"patternId\", lit(\"PatId1\")) \\\n","                               .withColumn(\"ActionType\", lit(\"UPGRADE\")) \\\n","                               .withColumn(\"MerchantId\", lit(merchant_id)) \\\n","                               .withColumn(\"customerName\", col(\"customerId\")) \\\n","                               .selectExpr(\"current_timestamp() as YStartTime\", \"current_timestamp() as detectionTime\", \"patternId\", \"ActionType\", \"customerName\", \"MerchantId\")\n","\n","        detections.append(upgrade_df)\n","\n","\n","    avg_txn_df = joined_df.groupBy(\"customerId\", \"merchantId\").agg(avg(\"amount\").alias(\"avg_amount\"), count(\"*\").alias(\"txn_count\"))\n","\n","\n","    avg_txn_df.write.jdbc(\n","        url=jdbc_url,\n","        table=\"customer_txn_summary\",\n","        mode=\"append\",\n","        properties=connection_properties\n","    )\n","\n","    child_df = avg_txn_df.filter((col(\"avg_amount\") < 23) & (col(\"txn_count\") >= 80)) \\\n","                         .withColumn(\"patternId\", lit(\"PatId2\")) \\\n","                         .withColumn(\"ActionType\", lit(\"CHILD\")) \\\n","                         .withColumn(\"customerName\", col(\"customerId\")) \\\n","                         .selectExpr(\"current_timestamp() as YStartTime\", \"current_timestamp() as detectionTime\", \"patternId\", \"ActionType\", \"customerName\", \"merchantId as MerchantId\")\n","\n","    detections.append(child_df)\n","\n","\n","    gender_df = joined_df.select(\"merchantId\", \"customerId\", \"gender\").dropDuplicates()\n","    gender_counts = gender_df.groupBy(\"merchantId\", \"gender\").agg(count(\"customerId\").alias(\"gender_count\"))\n","\n"," \n","    gender_counts.write.jdbc(\n","        url=jdbc_url,\n","        table=\"gender_counts\",\n","        mode=\"append\",\n","        properties=connection_properties\n","    )\n","\n","    gender_pivot = gender_counts.groupBy(\"merchantId\").pivot(\"gender\").sum(\"gender_count\").fillna(0)\n","    if \"Female\" in gender_pivot.columns and \"Male\" in gender_pivot.columns:\n","        dei_df = gender_pivot.filter((col(\"Female\") < col(\"Male\")) & (col(\"Female\") > 100)) \\\n","                             .withColumn(\"patternId\", lit(\"PatId3\")) \\\n","                             .withColumn(\"ActionType\", lit(\"DEI-NEEDED\")) \\\n","                             .withColumn(\"customerName\", lit(\"\")) \\\n","                             .selectExpr(\"current_timestamp() as YStartTime\", \"current_timestamp() as detectionTime\", \"patternId\", \"ActionType\", \"customerName\", \"merchantId as MerchantId\")\n","\n","        detections.append(dei_df)\n","\n","\n","if len(detections) == 0:\n","    print(\"No detections found. Check your pattern logic or input data.\")\n","else:\n","    final_df = detections[0]\n","    for df in detections[1:]:\n","        final_df = final_df.union(df)\n","\n","    print(\"Total detections:\", final_df.count())\n","    final_df.show(5)\n","\n","    total = final_df.count()\n","    batches = (total // 50) + 1\n","    for i in range(batches):\n","        batch_df = final_df.limit(50).offset(i * 50)\n","        try:\n","            batch_df.write.mode(\"overwrite\").json(\"Files/detections/detection_batch_{}.json\".format(i))\n","            print(f\"Batch {i} written successfully.\")\n","        except Exception as e:\n","            print(f\"Error writing batch {i}:\", e)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":26,"statement_ids":[26],"state":"finished","livy_statement_state":"available","session_id":"0ac6c71f-3d0d-454e-b47c-eccf207ac5f8","normalized_state":"finished","queued_time":"2025-08-10T15:42:10.9509552Z","session_start_time":null,"execution_start_time":"2025-08-10T15:42:10.952315Z","execution_finish_time":"2025-08-10T15:42:12.4777161Z","parent_msg_id":"5c28fdad-8a5d-45ba-84fe-a817820ff3d8"},"text/plain":"StatementMeta(, 0ac6c71f-3d0d-454e-b47c-eccf207ac5f8, 26, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["No detections found. Check your pattern logic or input data.\n"]}],"execution_count":24,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6ebdd368-de29-4735-bcb8-9713b0e9fc3c"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"5e7beabe-a64d-478b-a5ec-47cccc4ea716"}],"default_lakehouse":"5e7beabe-a64d-478b-a5ec-47cccc4ea716","default_lakehouse_name":"Transaction_Lakehouse","default_lakehouse_workspace_id":"9e108cca-f00a-4ed8-a107-b025c1d99e01"}}},"nbformat":4,"nbformat_minor":5}